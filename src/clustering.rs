//! Incremental clustering with optimal K heuristics for `ArrowSpace`.
//!
//! These functionalities are used to reduce the raw data input for the Laplacian
//! and spectral computations to clustered raw data.
//!
//! This module provides:
//! - `ClusteringHeuristic` trait: compute optimal K from NxF data
//! - `OptimalKHeuristic`: multi-step heuristic using intrinsic dimension,
//!   Calinski-Harabasz analysis, and adaptive thresholding
//! - Helper methods for distance computations and pilot k-means
//! - Parallel implementations for performance-critical operations
//! Incremental clustering with optimal K heuristics for `ArrowSpace`.
//!
//! **DETERMINISTIC**: All random operations use fixed seed 128 for reproducibility.

use log::{debug, info, warn};
use rand::seq::SliceRandom;
use rand::SeedableRng;
use rayon::prelude::*;
use smartcore::cluster::kmeans::{KMeans, KMeansParameters};
use smartcore::linalg::basic::arrays::Array2;
use smartcore::linalg::basic::matrix::DenseMatrix;

/// Fixed seed for deterministic clustering
const CLUSTERING_SEED: u64 = 128;

/// Trait for computing optimal clustering parameters from data.
pub trait ClusteringHeuristic {
    /// Compute optimal number of clusters K, squared-distance threshold radius,
    /// and estimated intrinsic dimension from NxF data matrix.
    fn compute_optimal_k(&self, rows: &[Vec<f64>], n: usize, f: usize) -> (usize, f64, usize)
    where
        Self: Sync,
    {
        info!("Computing optimal K for clustering: N={}, F={}", n, f);

        // Step 1: Establish bounds
        let (k_min, k_max, id_est) = self.step1_bounds(rows, n, f);
        info!(
            "step 1 bounds: K in [{}, {}], intrinsic_dim â‰ˆ {}",
            k_min, k_max, id_est
        );

        // Step 2: Spectral gap via Calinski-Harabasz
        let sample_size = n.min(1000);
        let sample_indices: Vec<usize> = if n > sample_size {
            let mut rng = rand::rngs::StdRng::seed_from_u64(CLUSTERING_SEED);
            let mut idxs: Vec<usize> = (0..n).collect();
            idxs.shuffle(&mut rng);
            idxs[..sample_size].to_vec()
        } else {
            (0..n).collect()
        };

        let sampled_rows: Vec<Vec<f64>> = sample_indices
            .par_iter()
            .map(|&i| rows[i].clone())
            .collect();

        let k_optimal = self.step2_calinski_harabasz(&sampled_rows, k_min, k_max);
        info!("step 2 Calinski-Harabasz suggests K = {}", k_optimal);

        // Step 3: Derive radius threshold
        let radius = self.compute_threshold_from_pilot(&sampled_rows, k_optimal);
        info!("Computed radius threshold: {:.6}", radius);

        (k_optimal, radius, id_est)
    }

    // Step 1: Bounds via N/F and intrinsic dimension
    fn step1_bounds(&self, rows: &[Vec<f64>], n: usize, f: usize) -> (usize, usize, usize) {
        let id_est = self.estimate_intrinsic_dimension(rows, n, f);
        debug!("Intrinsic dimension estimate: {}", id_est);

        let k_min = ((n as f64 / 10.0).sqrt().ceil() as usize).max(2);

        let k_max_candidates = vec![f, n / 10, 5 * id_est, (n as f64).powf(0.5) as usize];

        let k_max = k_max_candidates
            .iter()
            .copied()
            .min()
            .unwrap_or(f)
            .max(k_min + 1)
            .min(n / 2);

        (k_min, k_max, id_est)
    }

    /// Estimate intrinsic dimension via Two-NN ratio method (DETERMINISTIC).
    fn estimate_intrinsic_dimension(&self, rows: &[Vec<f64>], n: usize, f: usize) -> usize {
        if n < 10 {
            return f.min(2);
        }

        let sample_size = n.min(500);
        let mut rng = rand::rngs::StdRng::seed_from_u64(CLUSTERING_SEED.wrapping_add(1));
        let mut indices: Vec<usize> = (0..n).collect();
        indices.shuffle(&mut rng);
        let sample_indices = &indices[..sample_size];

        let ratios: Vec<f64> = sample_indices
            .par_iter()
            .filter_map(|&i| {
                let row_i = &rows[i];
                let mut dists: Vec<(usize, f64)> = (0..n)
                    .filter(|&j| j != i)
                    .map(|j| {
                        let d2: f64 = row_i
                            .iter()
                            .zip(&rows[j])
                            .map(|(a, b)| (a - b).powi(2))
                            .sum();
                        (j, d2.sqrt())
                    })
                    .collect();

                dists.sort_by(|a, b| a.1.partial_cmp(&b.1).unwrap());

                if dists.len() >= 2 {
                    let d1 = dists[0].1;
                    let d2 = dists[1].1;
                    if d1 > 1e-12 {
                        return Some(d2 / d1);
                    }
                }
                None
            })
            .collect();

        if ratios.is_empty() {
            return f.min(3);
        }

        let mean_ratio: f64 = ratios.iter().sum::<f64>() / ratios.len() as f64;
        let id = if mean_ratio > 1.001 {
            1.0 / mean_ratio.ln()
        } else {
            f as f64
        };
        let id_clamped = (id.round() as usize).clamp(1, f);

        debug!(
            "Two-NN mean ratio: {:.4}, estimated ID: {}",
            mean_ratio, id_clamped
        );
        id_clamped
    }

    // Step 2: Calinski-Harabasz for optimal K (DETERMINISTIC with full parallelism)
    fn step2_calinski_harabasz(&self, rows: &[Vec<f64>], k_min: usize, k_max: usize) -> usize
    where
        Self: Sync,
    {
        let n = rows.len();
        if n < 10 {
            return k_min;
        }

        let k_range = k_max - k_min;
        let k_step = if k_range <= 5 {
            1
        } else if k_range <= 15 {
            2
        } else {
            3
        };

        let k_candidates: Vec<usize> = (k_min..=k_max).step_by(k_step).collect();
        debug!(
            "Testing K in range [{}, {}] with step {}",
            k_min, k_max, k_step
        );

        // Parallel evaluation with deterministic seeds
        let k_scores: Vec<(usize, f64)> = k_candidates
            .par_iter()
            .filter(|&&k| k < n && k >= 2)
            .map(|&k| {
                let best_ch_for_k: f64 = (0..3)
                    .into_par_iter()
                    .map(|trial| {
                        // Derive unique seed: base + k*1000 + trial
                        let trial_seed = CLUSTERING_SEED
                            .wrapping_add((k as u64) * 1000)
                            .wrapping_add(trial as u64);

                        let assignments = kmeans_lloyd(rows, k, 20, trial_seed);
                        self.calinski_harabasz_score(rows, &assignments, k)
                    })
                    .max_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal))
                    .unwrap_or(0.0);

                let penalty = 0.8;
                let penalized_score = best_ch_for_k - penalty * (k as f64) * (n as f64).ln();

                debug!(
                    "K={}: CH={:.4}, penalized={:.4}",
                    k, best_ch_for_k, penalized_score
                );

                (k, penalized_score)
            })
            .collect();

        // DETERMINISTIC: Sequential max with conservative tiebreaker (prefer LARGER k)
        let (mut best_k, mut best_score) = k_scores
            .iter()
            .max_by(|(k_a, score_a), (k_b, score_b)| {
                // Primary: compare scores
                match score_a.partial_cmp(score_b) {
                    Some(std::cmp::Ordering::Greater) => std::cmp::Ordering::Greater,
                    Some(std::cmp::Ordering::Less) => std::cmp::Ordering::Less,
                    // Tiebreaker: prefer LARGER k (conservative for randomness)
                    _ => k_a.cmp(k_b),
                }
            })
            .map(|&(k, s)| (k, s))
            .unwrap_or((k_min, f64::NEG_INFINITY));

        // Fine-tune around best_k (if needed)
        if k_step > 1 {
            let fine_range: Vec<usize> = vec![
                best_k.saturating_sub(k_step - 1),
                best_k.saturating_sub(1),
                best_k,
                (best_k + 1).min(k_max),
                (best_k + k_step - 1).min(k_max),
            ]
            .into_iter()
            .filter(|&k| k >= k_min && k <= k_max && k < n && !k_candidates.contains(&k))
            .collect();

            // Parallel fine-tuning
            let fine_scores: Vec<(usize, f64)> = fine_range
                .par_iter()
                .map(|&k| {
                    let best_ch_for_k: f64 = (0..3)
                        .into_par_iter()
                        .map(|trial| {
                            // Fine-tuning seed: base + k*10000 + trial
                            let trial_seed = CLUSTERING_SEED
                                .wrapping_add((k as u64) * 10000)
                                .wrapping_add(trial as u64);

                            let assignments = kmeans_lloyd(rows, k, 20, trial_seed);
                            self.calinski_harabasz_score(rows, &assignments, k)
                        })
                        .max_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal))
                        .unwrap_or(0.0);

                    let penalty = 0.8;
                    let penalized_score = best_ch_for_k - penalty * (k as f64) * (n as f64).ln();

                    debug!(
                        "K={} (fine): CH={:.4}, penalized={:.4}",
                        k, best_ch_for_k, penalized_score
                    );

                    (k, penalized_score)
                })
                .collect();

            // DETERMINISTIC: Sequential max for fine-tuning results
            if let Some(&(fine_k, fine_score)) = fine_scores
                .iter()
                .max_by(|(k_a, score_a), (k_b, score_b)| {
                    match score_a.partial_cmp(score_b) {
                        Some(std::cmp::Ordering::Greater) => std::cmp::Ordering::Greater,
                        Some(std::cmp::Ordering::Less) => std::cmp::Ordering::Less,
                        // Tiebreaker: prefer LARGER k (conservative)
                        _ => k_a.cmp(k_b),
                    }
                })
            {
                if fine_score > best_score {
                    best_k = fine_k;
                    best_score = fine_score;
                }
            }
        }

        debug!("Best K={} with penalized score={:.4}", best_k, best_score);
        if best_k < k_max {
            best_k
        } else {
            k_max
        }
    }

    /// Calinski-Harabasz index (parallelized).
    fn calinski_harabasz_score(&self, rows: &[Vec<f64>], assignments: &[usize], k: usize) -> f64 {
        let n = rows.len();
        let f = rows[0].len();

        if k <= 1 || k >= n {
            return 0.0;
        }

        let global_centroid: Vec<f64> = (0..f)
            .into_par_iter()
            .map(|j| rows.iter().map(|row| row[j]).sum::<f64>() / n as f64)
            .collect();

        let centroids_counts: Vec<(Vec<f64>, usize)> = (0..k)
            .into_par_iter()
            .map(|c| {
                let mut centroid = vec![0.0; f];
                let mut count = 0;

                for (i, &cluster) in assignments.iter().enumerate() {
                    if cluster == c {
                        for j in 0..f {
                            centroid[j] += rows[i][j];
                        }
                        count += 1;
                    }
                }

                if count > 0 {
                    for val in &mut centroid {
                        *val /= count as f64;
                    }
                }
                (centroid, count)
            })
            .collect();

        let bgss: f64 = centroids_counts
            .par_iter()
            .filter(|(_, count)| *count > 0)
            .map(|(centroid, count)| {
                let d2: f64 = centroid
                    .iter()
                    .zip(&global_centroid)
                    .map(|(a, b)| (a - b).powi(2))
                    .sum();
                (*count as f64) * d2
            })
            .sum();

        let wgss: f64 = assignments
            .par_iter()
            .enumerate()
            .filter(|(_, c)| **c < k)
            .map(|(i, &c)| {
                rows[i]
                    .iter()
                    .zip(&centroids_counts[c].0)
                    .map(|(a, b)| (a - b).powi(2))
                    .sum::<f64>()
            })
            .sum();

        if wgss < 1e-10 {
            return 0.0;
        }

        (bgss / (k - 1) as f64) / (wgss / (n - k) as f64)
    }

    // Step 3: Adaptive threshold (DETERMINISTIC)
    fn compute_threshold_from_pilot(&self, rows: &[Vec<f64>], k: usize) -> f64 {
        let assignments = kmeans_lloyd(rows, k, 20, CLUSTERING_SEED.wrapping_add(100000));
        let f = rows[0].len();

        let centroids_counts: Vec<(Vec<f64>, usize)> = (0..k)
            .into_par_iter()
            .map(|c| {
                let mut centroid = vec![0.0; f];
                let mut count = 0;

                for (i, &cluster) in assignments.iter().enumerate() {
                    if cluster == c {
                        for j in 0..f {
                            centroid[j] += rows[i][j];
                        }
                        count += 1;
                    }
                }

                if count > 0 {
                    for val in &mut centroid {
                        *val /= count as f64;
                    }
                }
                (centroid, count)
            })
            .collect();

        let centroids: Vec<Vec<f64>> = centroids_counts.iter().map(|(c, _)| c.clone()).collect();
        let counts: Vec<usize> = centroids_counts.iter().map(|(_, cnt)| *cnt).collect();

        let mut dists: Vec<f64> = assignments
            .par_iter()
            .enumerate()
            .filter(|(_, c)| **c < k)
            .map(|(i, &c)| {
                rows[i]
                    .iter()
                    .zip(&centroids[c])
                    .map(|(a, b)| (a - b).powi(2))
                    .sum()
            })
            .collect();

        if dists.is_empty() {
            warn!("No distances computed; using default radius 1.0");
            return 1.0;
        }

        dists.sort_by(|a, b| a.partial_cmp(b).unwrap());
        let percentile_90_idx = ((dists.len() as f64 * 0.9).ceil() as usize).min(dists.len() - 1);
        let percentile_90 = dists[percentile_90_idx];

        let inter_dists: Vec<f64> = (0..k)
            .into_par_iter()
            .flat_map(|i| {
                ((i + 1)..k)
                    .into_par_iter()
                    .filter({
                        let value = counts.clone();
                        move |j| value[i] > 0 && value[*j] > 0
                    })
                    .map({
                        let value = centroids.clone();
                        move |j| {
                            value[i]
                                .iter()
                                .zip(&value[j])
                                .map(|(a, b)| (a - b).powi(2))
                                .sum()
                        }
                    })
                    .collect::<Vec<f64>>()
            })
            .collect();

        let min_inter_centroid_dist2 = if !inter_dists.is_empty() {
            inter_dists.iter().fold(f64::INFINITY, |a, &b| a.min(b))
        } else {
            f64::INFINITY
        };

        let threshold_ratio =
            if min_inter_centroid_dist2.is_finite() && min_inter_centroid_dist2 > 0.0 {
                percentile_90 / min_inter_centroid_dist2
            } else {
                1.0
            };

        if percentile_90 < 1e-8 || threshold_ratio < 0.01 {
            debug!(
                "Within-cluster variance very small (p90={:.3e}, ratio={:.3e}); using inter-centroid fallback",
                percentile_90, threshold_ratio
            );

            if !inter_dists.is_empty() {
                let radius = min_inter_centroid_dist2 * 0.15;
                debug!("Inter-centroid fallback: radius={:.6}", radius);
                return radius.max(1e-6);
            } else {
                debug!("All centroids identical; using minimum radius");
                return 1e-6;
            }
        }

        let radius = percentile_90 * 1.5;
        debug!("Standard threshold: radius={:.6}", radius);
        radius.max(1e-6)
    }
}

/// Perform K-Means clustering using Lloyd's algorithm
///
/// # Arguments
/// * `rows` - Input data as Vec<Vec<f64>> where each inner vec is a sample
/// * `k` - Number of clusters
/// * `max_iter` - Maximum iterations for convergence
/// * `seed` - Random seed for reproducibility
///
/// # Returns
/// Vector of cluster assignments (0-indexed)
pub fn kmeans_lloyd(rows: &[Vec<f64>], k: usize, max_iter: usize, seed: u64) -> Vec<usize> {
    if rows.is_empty() {
        return Vec::new();
    }

    let (n, f) = (rows.len(), rows[0].len());
    let k = k.min(n);

    // Flatten row-major data
    let x: DenseMatrix<f64> =
        DenseMatrix::from_iterator(rows.into_iter().flatten().map(|x| *x), n, f, 0);

    // Create parameters with explicit seed
    let params = KMeansParameters {
        k,
        max_iter,
        seed: Some(seed),
    };

    // Fit the model
    let km = KMeans::fit(&x, params).expect("Failed to fit K-Means model");

    // Predict cluster assignments
    let labels = km.predict(&x).expect("Failed to predict labels");

    labels
}

pub fn nearest_centroid(row: &[f64], centroids: &[Vec<f64>]) -> (usize, f64) {
    let mut best_idx = 0;
    let mut best_dist2 = f64::INFINITY;

    for (i, c) in centroids.iter().enumerate() {
        let mut d2 = 0.0;
        for (a, b) in row.iter().zip(c.iter()) {
            let diff = a - b;
            d2 += diff * diff;
        }

        if d2 < best_dist2 {
            best_dist2 = d2;
            best_idx = i;
        }
    }

    (best_idx, best_dist2)
}

pub fn euclidean_dist(a: &[f64], b: &[f64]) -> f64 {
    a.iter()
        .zip(b)
        .map(|(x, y)| (x - y).powi(2))
        .sum::<f64>()
        .sqrt()
}
