/// Top k-3 results ids should be 3, 6, 0 for every algorithm used
use arrowspace::builder::ArrowSpaceBuilder;
use arrowspace::core::ArrowItem;
use smartcore::linalg::basic::arrays::Array;

#[path = "./common/lib.rs"]
mod common;


const VECTORS_DATA: &str = r#"
P0001; 0.82,0.11,0.43,0.28,0.64,0.32,0.55,0.48,0.19,0.73,0.07,0.36,0.58,0.23,0.44,0.31,0.52,0.16,0.61,0.40,0.27,0.49,0.35,0.29
P0002; 0.79,0.12,0.45,0.29,0.61,0.33,0.54,0.47,0.21,0.70,0.08,0.37,0.56,0.22,0.46,0.30,0.51,0.18,0.60,0.39,0.26,0.48,0.36,0.30
P0003; 0.78,0.13,0.46,0.27,0.62,0.34,0.53,0.46,0.22,0.69,0.09,0.35,0.55,0.24,0.45,0.29,0.50,0.17,0.59,0.38,0.28,0.47,0.34,0.31
P0004; 0.81,0.10,0.44,0.26,0.63,0.31,0.56,0.45,0.20,0.71,0.06,0.34,0.57,0.25,0.47,0.33,0.53,0.15,0.62,0.41,0.25,0.50,0.37,0.27
P0005; 0.80,0.12,0.42,0.25,0.60,0.35,0.52,0.49,0.23,0.68,0.10,0.38,0.54,0.21,0.43,0.28,0.49,0.19,0.58,0.37,0.29,0.46,0.33,0.32
P0006; 0.77,0.14,0.41,0.24,0.59,0.36,0.51,0.50,0.24,0.67,0.11,0.39,0.53,0.20,0.42,0.27,0.48,0.20,0.57,0.36,0.30,0.45,0.32,0.33
P0007; 0.83,0.09,0.47,0.30,0.65,0.33,0.57,0.44,0.18,0.72,0.05,0.33,0.59,0.26,0.48,0.34,0.54,0.14,0.63,0.42,0.24,0.51,0.38,0.26
P0008; 0.76,0.15,0.40,0.23,0.58,0.37,0.50,0.51,0.25,0.66,0.12,0.40,0.52,0.19,0.41,0.26,0.47,0.21,0.56,0.35,0.31,0.44,0.31,0.34
P0009; 0.75,0.16,0.39,0.22,0.57,0.38,0.49,0.52,0.26,0.65,0.13,0.41,0.51,0.18,0.40,0.25,0.46,0.22,0.55,0.34,0.32,0.43,0.30,0.35
P0010; 0.84,0.08,0.48,0.31,0.66,0.32,0.58,0.43,0.17,0.74,0.04,0.32,0.60,0.27,0.49,0.35,0.55,0.13,0.64,0.43,0.23,0.52,0.39,0.25
P0011; 0.72,0.18,0.37,0.21,0.55,0.39,0.47,0.54,0.27,0.63,0.15,0.42,0.49,0.17,0.39,0.24,0.45,0.23,0.54,0.33,0.33,0.42,0.29,0.36
P00112; 0.73,0.17,0.38,0.20,0.56,0.40,0.48,0.53,0.28,0.64,0.14,0.43,0.50,0.16,0.38,0.23,0.44,0.24,0.53,0.32,0.34,0.41,0.28,0.37
P0013; 0.71,0.19,0.36,0.19,0.54,0.41,0.46,0.55,0.29,0.62,0.16,0.44,0.48,0.15,0.37,0.22,0.43,0.25,0.52,0.31,0.35,0.40,0.27,0.38
P0014; 0.85,0.07,0.49,0.32,0.67,0.31,0.59,0.42,0.16,0.75,0.03,0.31,0.61,0.28,0.50,0.36,0.56,0.12,0.65,0.44,0.22,0.53,0.40,0.24
P0015; 0.70,0.20,0.35,0.18,0.53,0.42,0.45,0.56,0.30,0.61,0.17,0.45,0.47,0.14,0.36,0.21,0.42,0.26,0.51,0.30,0.36,0.39,0.26,0.39
P0016; 0.69,0.21,0.34,0.17,0.52,0.43,0.44,0.57,0.31,0.60,0.18,0.46,0.46,0.13,0.35,0.20,0.41,0.27,0.50,0.29,0.37,0.38,0.25,0.40
P0017; 0.86,0.06,0.50,0.33,0.68,0.30,0.60,0.41,0.15,0.76,0.02,0.30,0.62,0.29,0.51,0.37,0.57,0.11,0.66,0.45,0.21,0.54,0.41,0.23
P0018; 0.68,0.22,0.33,0.16,0.51,0.44,0.43,0.58,0.32,0.59,0.19,0.47,0.45,0.12,0.34,0.19,0.40,0.28,0.49,0.28,0.38,0.37,0.24,0.41
P0019; 0.67,0.23,0.32,0.15,0.50,0.45,0.42,0.59,0.33,0.58,0.20,0.48,0.44,0.11,0.33,0.18,0.39,0.29,0.48,0.27,0.39,0.36,0.23,0.42
P0020; 0.87,0.05,0.51,0.34,0.69,0.29,0.61,0.40,0.14,0.77,0.01,0.29,0.63,0.30,0.52,0.38,0.58,0.10,0.67,0.46,0.20,0.55,0.42,0.22
P0021; 0.66,0.24,0.31,0.14,0.49,0.46,0.41,0.60,0.34,0.57,0.21,0.49,0.43,0.10,0.32,0.17,0.38,0.30,0.47,0.26,0.40,0.35,0.22,0.43
P0022; 0.65,0.25,0.30,0.13,0.48,0.47,0.40,0.61,0.35,0.56,0.22,0.50,0.42,0.09,0.31,0.16,0.37,0.31,0.46,0.25,0.41,0.34,0.21,0.44
P0023; 0.64,0.26,0.29,0.12,0.47,0.48,0.39,0.62,0.36,0.55,0.23,0.51,0.41,0.08,0.30,0.15,0.36,0.32,0.45,0.24,0.42,0.33,0.20,0.45
P0024; 0.88,0.04,0.52,0.35,0.70,0.28,0.62,0.39,0.13,0.78,0.00,0.28,0.64,0.31,0.53,0.39,0.59,0.09,0.68,0.47,0.19,0.56,0.43,0.21
P0025; 0.63,0.27,0.28,0.11,0.46,0.49,0.38,0.63,0.37,0.54,0.24,0.52,0.40,0.07,0.29,0.14,0.35,0.33,0.44,0.23,0.43,0.32,0.19,0.46
P0026; 0.62,0.28,0.27,0.10,0.45,0.50,0.37,0.64,0.38,0.53,0.25,0.53,0.39,0.06,0.28,0.13,0.34,0.34,0.43,0.22,0.44,0.31,0.18,0.47
P0027; 0.61,0.29,0.26,0.09,0.44,0.51,0.36,0.65,0.39,0.52,0.26,0.54,0.38,0.05,0.27,0.12,0.33,0.35,0.42,0.21,0.45,0.30,0.17,0.48
P0028; 0.60,0.30,0.25,0.08,0.43,0.52,0.35,0.66,0.40,0.51,0.27,0.55,0.37,0.04,0.26,0.11,0.32,0.36,0.41,0.20,0.46,0.29,0.16,0.49
P0029; 0.59,0.31,0.24,0.07,0.42,0.53,0.34,0.67,0.41,0.50,0.28,0.56,0.36,0.03,0.25,0.10,0.31,0.37,0.40,0.19,0.47,0.28,0.15,0.50
P0030; 0.58,0.32,0.23,0.06,0.41,0.54,0.33,0.68,0.42,0.49,0.29,0.57,0.35,0.02,0.24,0.09,0.30,0.38,0.39,0.18,0.48,0.27,0.14,0.51
P0031; 0.90,0.06,0.44,0.36,0.72,0.33,0.55,0.41,0.20,0.79,0.05,0.35,0.60,0.26,0.46,0.31,0.54,0.16,0.62,0.42,0.27,0.49,0.35,0.29
P0032; 0.57,0.33,0.22,0.05,0.40,0.55,0.32,0.69,0.43,0.48,0.30,0.58,0.34,0.01,0.23,0.08,0.29,0.39,0.38,0.17,0.49,0.26,0.13,0.52
P0033; 0.56,0.34,0.21,0.04,0.39,0.56,0.31,0.70,0.44,0.47,0.31,0.59,0.33,0.00,0.22,0.07,0.28,0.40,0.37,0.16,0.50,0.25,0.12,0.53
P0034; 0.55,0.35,0.20,0.03,0.38,0.57,0.30,0.71,0.45,0.46,0.32,0.60,0.32,0.02,0.21,0.06,0.27,0.41,0.36,0.15,0.51,0.24,0.11,0.54
P0035; 0.54,0.36,0.19,0.02,0.37,0.58,0.29,0.72,0.46,0.45,0.33,0.61,0.31,0.03,0.20,0.05,0.26,0.42,0.35,0.14,0.52,0.23,0.10,0.55
P0036; 0.53,0.37,0.18,0.01,0.36,0.59,0.28,0.73,0.47,0.44,0.34,0.62,0.30,0.04,0.19,0.04,0.25,0.43,0.34,0.13,0.53,0.22,0.09,0.56
P0037; 0.91,0.07,0.45,0.37,0.73,0.34,0.56,0.40,0.21,0.80,0.06,0.36,0.61,0.27,0.47,0.32,0.55,0.17,0.63,0.41,0.28,0.50,0.36,0.28
P0038; 0.52,0.38,0.17,0.00,0.35,0.60,0.27,0.74,0.48,0.43,0.35,0.63,0.29,0.05,0.18,0.03,0.24,0.44,0.33,0.12,0.54,0.21,0.08,0.57
P0039; 0.51,0.39,0.16,0.02,0.34,0.61,0.26,0.75,0.49,0.42,0.36,0.64,0.28,0.06,0.17,0.02,0.23,0.45,0.32,0.11,0.55,0.20,0.07,0.58
P0040; 0.50,0.40,0.15,0.03,0.33,0.62,0.25,0.76,0.50,0.41,0.37,0.65,0.27,0.07,0.16,0.01,0.22,0.46,0.31,0.10,0.56,0.19,0.06,0.59
P0041; 0.49,0.41,0.14,0.04,0.32,0.63,0.24,0.77,0.51,0.40,0.38,0.66,0.26,0.08,0.15,0.00,0.21,0.47,0.30,0.09,0.57,0.18,0.05,0.60
P0042; 0.48,0.42,0.13,0.05,0.31,0.64,0.23,0.78,0.52,0.39,0.39,0.67,0.25,0.09,0.14,0.02,0.20,0.48,0.29,0.08,0.58,0.17,0.04,0.61
P0043; 0.47,0.43,0.12,0.06,0.30,0.65,0.22,0.79,0.53,0.38,0.40,0.68,0.24,0.10,0.13,0.03,0.19,0.49,0.28,0.07,0.59,0.16,0.03,0.62
P0044; 0.46,0.44,0.11,0.07,0.29,0.66,0.21,0.80,0.54,0.37,0.41,0.69,0.23,0.11,0.12,0.04,0.18,0.50,0.27,0.06,0.60,0.15,0.02,0.63
P0045; 0.45,0.45,0.10,0.08,0.28,0.67,0.20,0.81,0.55,0.36,0.42,0.70,0.22,0.12,0.11,0.05,0.17,0.51,0.26,0.05,0.61,0.14,0.01,0.64
P0046; 0.44,0.46,0.09,0.09,0.27,0.68,0.19,0.82,0.56,0.35,0.43,0.71,0.21,0.13,0.10,0.06,0.16,0.52,0.25,0.04,0.62,0.13,0.00,0.65
P0047; 0.43,0.47,0.08,0.10,0.26,0.69,0.18,0.83,0.57,0.34,0.44,0.72,0.20,0.14,0.09,0.07,0.15,0.53,0.24,0.03,0.63,0.12,0.01,0.66
P0048; 0.42,0.48,0.07,0.11,0.25,0.70,0.17,0.84,0.58,0.33,0.45,0.73,0.19,0.15,0.08,0.08,0.14,0.54,0.23,0.02,0.64,0.11,0.02,0.67
P0049; 0.41,0.49,0.06,0.12,0.24,0.71,0.16,0.85,0.59,0.32,0.46,0.74,0.18,0.16,0.07,0.09,0.13,0.55,0.22,0.01,0.65,0.10,0.03,0.68
P0050; 0.40,0.50,0.05,0.13,0.23,0.72,0.15,0.86,0.60,0.31,0.47,0.75,0.17,0.17,0.06,0.10,0.12,0.56,0.21,0.00,0.66,0.09,0.04,0.69
P0051; 0.89,0.09,0.46,0.38,0.71,0.35,0.57,0.39,0.22,0.77,0.07,0.37,0.62,0.25,0.48,0.30,0.56,0.18,0.64,0.40,0.29,0.51,0.37,0.27
P0052; 0.39,0.51,0.04,0.14,0.22,0.73,0.14,0.87,0.61,0.30,0.48,0.76,0.16,0.18,0.05,0.11,0.11,0.57,0.20,0.02,0.67,0.08,0.05,0.70
P0053; 0.38,0.52,0.03,0.15,0.21,0.74,0.13,0.88,0.62,0.29,0.49,0.77,0.15,0.19,0.04,0.12,0.10,0.58,0.19,0.03,0.68,0.07,0.06,0.71
P0054; 0.37,0.53,0.02,0.16,0.20,0.75,0.12,0.89,0.63,0.28,0.50,0.78,0.14,0.20,0.03,0.13,0.09,0.59,0.18,0.04,0.69,0.06,0.07,0.72
P0055; 0.36,0.54,0.01,0.17,0.19,0.76,0.11,0.90,0.64,0.27,0.51,0.79,0.13,0.21,0.02,0.14,0.08,0.60,0.17,0.05,0.70,0.05,0.08,0.73
P0056; 0.35,0.55,0.00,0.18,0.18,0.77,0.10,0.91,0.65,0.26,0.52,0.80,0.12,0.22,0.01,0.15,0.07,0.61,0.16,0.06,0.71,0.04,0.09,0.74
P0057; 0.34,0.56,0.02,0.19,0.17,0.78,0.09,0.92,0.66,0.25,0.53,0.81,0.11,0.23,0.00,0.16,0.06,0.62,0.15,0.07,0.72,0.03,0.10,0.75
P0058; 0.33,0.57,0.03,0.20,0.16,0.79,0.08,0.93,0.67,0.24,0.54,0.82,0.10,0.24,0.01,0.17,0.05,0.63,0.14,0.08,0.73,0.02,0.11,0.76
P0059; 0.32,0.58,0.04,0.21,0.15,0.80,0.07,0.94,0.68,0.23,0.55,0.83,0.09,0.25,0.02,0.18,0.04,0.64,0.13,0.09,0.74,0.01,0.12,0.77
P0060; 0.31,0.59,0.05,0.22,0.14,0.81,0.06,0.95,0.69,0.22,0.56,0.84,0.08,0.26,0.03,0.19,0.03,0.65,0.12,0.10,0.75,0.00,0.13,0.78
P0061; 0.93,0.06,0.52,0.29,0.74,0.27,0.61,0.38,0.15,0.81,0.03,0.28,0.64,0.31,0.53,0.39,0.58,0.12,0.67,0.46,0.20,0.56,0.43,0.21
P0062; 0.30,0.60,0.06,0.23,0.13,0.82,0.05,0.96,0.70,0.21,0.57,0.85,0.07,0.27,0.04,0.20,0.02,0.66,0.11,0.11,0.76,0.01,0.14,0.79
P0063; 0.29,0.61,0.07,0.24,0.12,0.83,0.04,0.97,0.71,0.20,0.58,0.86,0.06,0.28,0.05,0.21,0.01,0.67,0.10,0.12,0.77,0.02,0.15,0.80
P0064; 0.28,0.62,0.08,0.25,0.11,0.84,0.03,0.98,0.72,0.19,0.59,0.87,0.05,0.29,0.06,0.22,0.00,0.68,0.09,0.13,0.78,0.03,0.16,0.81
"#;

fn main() {
    // uncomment this and set RUST_LOG=debug for debug
    // arrowspace::init();
    
    // Parse items as rows (N×24): each row is one protein with 24 features
    let (ids, db): (Vec<String>, Vec<Vec<f64>>) = common::parse_vectors_string(VECTORS_DATA);
    let n_items = db.len();

    // Query similar to item at index 3; scale slightly for testing
    let q_index = 3;
    let mut query = db[q_index].clone();
    for v in query.iter_mut() {
        *v *= 1.02;
    }

    let k = 8;

    // ----------------------------
    // Baseline: plain cosine KNN
    // ----------------------------
    let mut base_scores: Vec<(usize, f64)> = db
        .iter()
        .enumerate()
        .map(|(i, v)| (i, common::cosine_sim(&query, v)))
        .collect();
    base_scores.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());
    base_scores.truncate(k + 1);

    println!("Baseline cosine top-{}+1:", k);
    for (rank, (i, s)) in base_scores.iter().enumerate() {
        println!("  {}. idx={} ({}) score={:.6}", rank + 1, i, ids[*i], s);
    }

    // ---------------------------------------------------
    // ArrowSpace: build λ-graph from N×24 data
    // ---------------------------------------------------
    let eps = 1e-3;
    let graph_k = 20;
    let topk = k;
    let p = 2.0;
    let sigma_override = Some(1e-3 * 0.75);
    
    let (aspace, gl) = ArrowSpaceBuilder::new()
        .with_lambda_graph(eps, graph_k, topk, p, sigma_override)
        .with_normalisation(false)
        .with_dims_reduction(true, None)
        .build(db);

    println!("\nArrowSpace shape: {:?}", aspace.data.shape());
    assert_eq!(aspace.data.shape(), (n_items, 24));

    // Prepare query as ArrowItem
    // Note: query lambda is set to 0.0 initially; it will be computed for hybrid search
    let mut query_item = ArrowItem::new(query.clone(), 0.0);
    query_item.lambda = aspace.prepare_query_item(&query_item.item, &gl);

    println!("\n=============================================");
    println!("USING search_lambda_aware() METHOD");
    println!("=============================================");

    // ----------------------------
    // Test 1: Pure cosine (alpha=1.0)
    // ----------------------------
    println!("\n--- Test 1: Pure Cosine Similarity (alpha=1.0) ---");
    let alpha = 1.0;
    let results_cosine = aspace.search_lambda_aware(&query_item, k + 1, alpha);

    println!("ArrowSpace search_lambda_aware (alpha={}) top-{}+1:", alpha, k);
    for (rank, (i, s)) in results_cosine.iter().enumerate() {
        println!("  {}. idx={} ({}) score={:.6}", rank + 1, i, ids[*i], s);
    }

    // Verify agreement with baseline
    let ids_base: Vec<usize> = base_scores.iter().map(|x| x.0).collect();
    let ids_arrow_cos: Vec<usize> = results_cosine.iter().map(|x| x.0).collect();
    println!(
        "\nMatch (baseline vs Arrow cosine): {}",
        if ids_base == ids_arrow_cos {
            "OK"
        } else {
            "DIFF"
        }
    );

    // ----------------------------
    // Test 2: Lambda-aware (alpha=0.9)
    // ----------------------------
    println!("\n--- Test 2: Lambda-Aware Search (alpha=0.9) ---");
    let alpha = 0.9;
    let results_lambda = aspace.search_lambda_aware(&query_item, k + 5, alpha);

    println!("ArrowSpace search_lambda_aware (alpha={}) top-{}+5:", alpha, k);
    for (rank, (i, s)) in results_lambda.iter().enumerate() {
        println!("  {}. idx={} ({}) score={:.6}", rank + 1, i, ids[*i], s);
    }

    // Jaccard similarity with baseline
    let ids_arrow_lam: Vec<usize> = results_lambda.iter().take(k + 1).map(|x| x.0).collect();
    let s1: std::collections::BTreeSet<_> = ids_base.iter().copied().collect();
    let s2: std::collections::BTreeSet<_> = ids_arrow_lam.iter().copied().collect();
    let inter = s1.intersection(&s2).count() as f64;
    let union = s1.union(&s2).count() as f64;
    let jaccard = if union > 0.0 { inter / union } else { 1.0 };
    println!("Jaccard(baseline vs λ-aware): {:.3}", jaccard);

    // ----------------------------
    // Test 3: Sweep alpha values
    // ----------------------------
    println!("\n--- Test 3: Alpha Sweep (1.0 → 0.0) ---");
    println!("This shows how results transition from pure cosine to pure lambda-based");
    
    for alpha in [0.8, 0.7, 0.6, 0.55, 0.4].iter() {
        let results = aspace.search_lambda_aware(&query_item, topk, *alpha);

        println!("\nAlpha={:.1} ({}% cosine, {}% lambda):", 
                 alpha, (alpha * 100.0) as i32, ((1.0 - alpha) * 100.0) as i32);
        for (rank, (i, s)) in results.iter().enumerate() {
            println!("  {}. idx={:2} ({:6}) score={:.6}", rank + 1, i, ids[*i], s);
        }
    }

    // ----------------------------
    // Test 4: Compare with manual implementation
    // ----------------------------
    println!("\n--- Test 4: Verification Against Manual Implementation ---");
    let alpha = 0.7;
    
    // Using search_lambda_aware
    let auto_results = aspace.search_lambda_aware(&query_item, k, alpha);
    
    // Manual implementation for comparison
    let mut manual_results: Vec<(usize, f64)> = (0..n_items)
        .map(|i| {
            let item = aspace.get_item(i);
            let lambda = aspace.lambdas()[i];
            let item_row = ArrowItem::new(item.item.clone(), lambda);
            let score = query_item.lambda_similarity(&item_row, alpha);
            (i, score)
        })
        .collect();
    manual_results.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());
    manual_results.truncate(k);

    println!("Comparison at alpha={}:", alpha);
    println!("\nAutomatic (search_lambda_aware):");
    for (rank, (i, s)) in auto_results.iter().enumerate() {
        println!("  {}. idx={} ({}) score={:.6}", rank + 1, i, ids[*i], s);
    }

    println!("\nManual:");
    for (rank, (i, s)) in manual_results.iter().enumerate() {
        println!("  {}. idx={} ({}) score={:.6}", rank + 1, i, ids[*i], s);
    }

    // Check if results match
    let auto_ids: Vec<usize> = auto_results.iter().map(|x| x.0).collect();
    let manual_ids: Vec<usize> = manual_results.iter().map(|x| x.0).collect();
    println!(
        "\nImplementations match: {}",
        if auto_ids == manual_ids {
            "✓ YES"
        } else {
            "✗ NO"
        }
    );

    // ----------------------------
    // Performance comparison
    // ----------------------------
    println!("\n--- Test 5: Performance Comparison ---");
    use std::time::Instant;

    let iterations = 100;
    
    // Benchmark search_lambda_aware (parallel)
    let start = Instant::now();
    for _ in 0..iterations {
        let _ = aspace.search_lambda_aware(&query_item, k, 0.7);
    }
    let parallel_time = start.elapsed();

    // Benchmark manual (sequential)
    let start = Instant::now();
    for _ in 0..iterations {
        let mut results: Vec<(usize, f64)> = (0..n_items)
            .map(|i| {
                let item = aspace.get_item(i);
                let lambda = aspace.lambdas()[i];
                let item_row = ArrowItem::new(item.item.clone(), lambda);
                (i, query_item.lambda_similarity(&item_row, 0.7))
            })
            .collect();
        results.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());
        results.truncate(k);
    }
    let sequential_time = start.elapsed();

    println!("Performance over {} iterations:", iterations);
    println!("  Parallel (search_lambda_aware): {:?}", parallel_time);
    println!("  Sequential (manual loop):        {:?}", sequential_time);
    println!(
        "  Speedup: {:.2}x",
        sequential_time.as_secs_f64() / parallel_time.as_secs_f64()
    );

    println!("\n=============================================");
    println!("ALL TESTS COMPLETED");
    println!("=============================================");
}
